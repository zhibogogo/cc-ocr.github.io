<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy">
  <meta name="keywords" content="LLMs, GeoEval">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/ucas_logo_circle.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://arxiv.org/pdf/2402.10104v2">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> 
      <!-- @PAN TODO: consider adding links? -->
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/AlibabaResearch/AdvancedLiterateMachinery">
            <b>OCR</b>
          </a>
          <a class="navbar-item" href="https://github.com/QwenLM/Qwen2-VL">
            <b>LMMs</b>
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy</h1>


          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Alibaba Group,</span>
            <span class="author-block"><sup>2</sup>South China University of Technology</span>
            <br>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.02210"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Benchmarks/CC-OCR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>code</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AlibabaResearch/AdvancedLiterateMachinery/blob/main/Benchmarks/CC-OCR/data/README.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>dataset</span>
                  </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



 <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="https://img.alicdn.com/imgextra/i2/O1CN01B3N8lf1jfrPii5EoI_!!6000000004576-2-tps-4082-2113.png" alt="leaderboard" width="99%"/>
      <p> The performance of models across various subsets, revealing distinct strengths. Gemini-1.5-Pro and Qwen2-VL-72B are the two top-performing models. 
        Gemini-1.5-Pro achieves first in the multi-scene OCR, multilingual OCR, and document parsing tracks, while Qwen2-VL-72B takes first place in the KIE track and second place in the multi-scene OCR and document parsing tracks.
      </p>
    </div>
  </div>
</section> 




<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            The <b>CC-OCR<b> benchmark is specifically designed to evaluate the OCR capabilities of multimodal large models. 
            It draws from publicly available specialized task data, as well as carefully curated representative data from various real-world application scenarios. 
            <b>CC-OCR<b> includes a range of core OCR tasks while also focusing on the challenges and difficulties that arise in applications. 
              The benchmark covers most key perception tasks in the fields of image text and image documents. 
              One of the philosophy behind this work is that high accuracy in perception is the foundation of multimodal cognition.
              <b>CC-OCR<b> comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, formula recognition, document parsing, table parsing, 
                and key information extraction. 
                It includes 39 subsets with 7,058 full annotated images, of which 41% are sourced from real applications, being released for the first time. 
                This means that the benchmark can better assess the zero-shot recognition capabilities of large models.
          </p>
          <img src="https://img.alicdn.com/imgextra/i3/O1CN01Z4W4qP1ULVZvX3MPh_!!6000000002501-2-tps-4035-2080.png" alt="datasets" width="99%"/>
          <p>
            The main features of <b>CC-OCR<b> include:
          </p>
          <ul class="custom-list">
            <li><b>Four OCR-centric Tasks:</b> Multi-Scene Text Reading, Multilingual Text Reading, Document Parsing, and Visual Information Extraction.</li>
            <li><b>Fine-grained visual challenges:</b> TMulti-orientation, multi-scale, wild-scene noise, and various text fonts.</li>
            <li><b>Well annotated:</b> For OCR, both textual labels and positional boxes are annotated. For Parsing, LaTeX and HTML formats are applicable for documents and tables respectively. For KIE, JSON format is adopted..</li>
          </ul>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">

      

        <h2 class="title is-3" id="leaderboard_test">Leaderboard on GeoEval</h2>
        <div class="content">
          <p class="mt-3">Accuracy scores of models on our <b>GeoEval</b> benchmark</p>

          <table class="js-sort-table" id="results">
            <tr>
              <th class="js-sort-number">Model</th>
              <th class="js-sort-number">Multi-Scene OCR</th>
              <th class="js-sort-number">Multilingual OCR</th>
              <th class="js-sort-number">Document Parsing</th>
              <th class="js-sort-number">Key Information Extraction</th>
            </tr>
            <tr>
              <td>TextMonkey</td>
              <td>56.9</td>
              <td>n/a</td>
              <td>n/a</td>
              <td>n/a</td>
            </tr>
            <tr>
              <td>KOSMOS2.5</td>
              <td>47.5</td>
              <td>36.2</td>
              <td>n/a</td>
              <td>n/a</td>
            </tr>
            <tr>
              <td>Florence</td>
              <td>49.2</td>
              <td>49.7</td>
              <td>n/a</td>
              <td>n/a</td>
            </tr>
            <tr>
              <td>GOT</td>
              <td>61.0</b></td>
              <td>24.9</td>
              <td>39.2</td>
              <td>n/a</td>
            </tr>
            <tr>
              <td>InternVL2-76B</td>
              <td>76.9</td>
              <td>46.6</td>
              <td>35.3</td>
              <td>61.6</td>
            </tr>
            <tr>
              <td>Claude3.5-sonnet</td>
              <td>72.9</td>
              <td>65.7</td>
              <td>47.8</td>
              <td>64.6</td>
            </tr>
            <tr>
              <td>GPT-4O</td>
              <td>76.4</td>
              <td>73.4</td>
              <td>53.3</td>
              <td>63.5</td>
            </tr>
            <tr>
              <td>Qwen2-VL-72B</td>
              <td>78.0</td>
              <td>71.1</td>
              <td>53.8</td>
              <td><b class="best-score-text">71.8</b></td>
            </tr>
            <tr>
              <td>Gemini-1.5-pro</td>
              <td><b class="best-score-text">83.2</b></td>
              <td><b class="best-score-text">79.0</b></td>
              <td><b class="best-score-text">62.4</b></td>
              <td>67.3</td>
            </tr>
                                             
        </table>
          <p>
            We evaluate nine representative LMMs either with open-source models or commercial APIs. The commercial APIs with specific versions are 
            GPT-4o-2024-08-06, Gemini-1.5-Pro-002, Claude-3.5-Sonnet-20241022.The open-source LMMs include KOSMOS2.5, 
            TextMonkey,Florence, GOT, InternVL2-76B, and Qwen2-VL-72B.
          </p>
          <p>
            🚨 For more details, please refer to <a href="https://arxiv.org/pdf/2412.02210">this link</a> 
          </p>
          </div>
        </div>

      </div>
    </div>

  </div>
</section>



<section class="section">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Examples</h2>
        <p>Examples from different perspectives in
          <span class="mathvista">CC-OCR</span></p>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="https://img.alicdn.com/imgextra/i3/O1CN01aNiMTY1VzdtmO57TO_!!6000000002724-2-tps-1386-1006.png" alt="example" width="50%"/>
              <p> Example of Multi-Scene Text Recognition</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="https://img.alicdn.com/imgextra/i4/O1CN01p0449R1fLTJqRn0gL_!!6000000003990-2-tps-1384-1102.png" alt="example" width="50%"/>
              <p> Example of Multi-Scene Text Recognition</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="https://img.alicdn.com/imgextra/i4/O1CN011KZDTy1YProHA1Lcf_!!6000000003052-2-tps-1384-968.png" alt="example" width="50%"/>
              <p> Example of Text Grounding</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="https://img.alicdn.com/imgextra/i4/O1CN01OGYW861cRTfMVwBep_!!6000000003597-2-tps-1384-1230.png" alt="example" width="60%"/>
              <p>Example of Document Parsing</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="https://img.alicdn.com/imgextra/i2/O1CN01wQUvUy1gqRilSBb4H_!!6000000004193-2-tps-1392-1148.png" alt="example" width="60%"/>
              <p>Example of Table Parsing</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="https://img.alicdn.com/imgextra/i1/O1CN01GVXfSZ1N7aE2VMZFZ_!!6000000001523-2-tps-1382-1528.png" alt="example" width="60%"/>
              <p>Example of Formula Parsing</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="https://img.alicdn.com/imgextra/i4/O1CN01cROwrT1iziyb6dlad_!!6000000004484-2-tps-1728-1522.png" alt="example" width="60%"/>
              <p>Example of KIE</p>
            </div>
          </div>
         

          
        </div>

        <br><br>
      
      </div>
    </div>

  </section>    

<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@misc{yang2024ccocrcomprehensivechallengingocr,
      title={CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Literacy}, 
      author={Zhibo Yang and Jun Tang and Zhaohai Li and Pengfei Wang and Jianqiang Wan and Humen Zhong and Xuejing Liu and Mingkun Yang and Peng Wang and Shuai Bai and LianWen Jin and Junyang Lin},
      year={2024},
      eprint={2412.02210},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.02210}, 
}</code></pre>
  </div>
</section>



<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
