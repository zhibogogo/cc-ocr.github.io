<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>From Pixels to Pages: Evaluating the Reading Powers of Large Models！ | Qwen</title>
<meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE Paper
The ability to read and write is a unique trait of humans and a hallmark of advanced intelligence. That&rsquo;s why it&rsquo;s becoming one of the essential skills that Large Multimodal Models ( LMMs ) must possess as they strive for human-like intelligence. LMMs have demonstrated impressive performance in recognizing document images with natural language instructions. However, as we dive deeper into more real-world scenarios, we find that challenges still remain."><meta name=author content="Qwen Team"><link rel=canonical href=http://localhost:1313/blog/cc-ocr-bench/><link crossorigin=anonymous href=/assets/css/stylesheet.09368503f3694f7ead33b6239bc528583b3431b7837d0401ce3b2c7fd4b0f5f1.css integrity="sha256-CTaFA/NpT36tM7Yjm8UoWDs0MbeDfQQBzjssf9Sw9fE=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/cc-ocr-bench/><link rel=alternate hreflang=zh href=http://localhost:1313/zh/blog/cc-ocr-bench/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="From Pixels to Pages: Evaluating the Reading Powers of Large Models！"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE Paper
The ability to read and write is a unique trait of humans and a hallmark of advanced intelligence. That&rsquo;s why it&rsquo;s becoming one of the essential skills that Large Multimodal Models ( LMMs ) must possess as they strive for human-like intelligence. LMMs have demonstrated impressive performance in recognizing document images with natural language instructions. However, as we dive deeper into more real-world scenarios, we find that challenges still remain."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/cc-ocr-bench/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-12-06T18:40:04+08:00"><meta property="article:modified_time" content="2024-12-06T18:40:04+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="From Pixels to Pages: Evaluating the Reading Powers of Large Models！"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE Paper
The ability to read and write is a unique trait of humans and a hallmark of advanced intelligence. That&rsquo;s why it&rsquo;s becoming one of the essential skills that Large Multimodal Models ( LMMs ) must possess as they strive for human-like intelligence. LMMs have demonstrated impressive performance in recognizing document images with natural language instructions. However, as we dive deeper into more real-world scenarios, we find that challenges still remain."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"http://localhost:1313/blog/"},{"@type":"ListItem","position":2,"name":"From Pixels to Pages: Evaluating the Reading Powers of Large Models！","item":"http://localhost:1313/blog/cc-ocr-bench/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"From Pixels to Pages: Evaluating the Reading Powers of Large Models！","name":"From Pixels to Pages: Evaluating the Reading Powers of Large Models！","description":"GITHUB HUGGING FACE MODELSCOPE Paper\nThe ability to read and write is a unique trait of humans and a hallmark of advanced intelligence. That\u0026rsquo;s why it\u0026rsquo;s becoming one of the essential skills that Large Multimodal Models ( LMMs ) must possess as they strive for human-like intelligence. LMMs have demonstrated impressive performance in recognizing document images with natural language instructions. However, as we dive deeper into more real-world scenarios, we find that challenges still remain.","keywords":[],"articleBody":"GITHUB HUGGING FACE MODELSCOPE Paper\nThe ability to read and write is a unique trait of humans and a hallmark of advanced intelligence. That’s why it’s becoming one of the essential skills that Large Multimodal Models ( LMMs ) must possess as they strive for human-like intelligence. LMMs have demonstrated impressive performance in recognizing document images with natural language instructions. However, as we dive deeper into more real-world scenarios, we find that challenges still remain. How do we measure to what extent large models can effectively handle complex and richly detailed images that require fine perception? This is definitely a question worth exploring and pondering.\nThe current landscape lacks a comprehensive benchmark to effectively measure the literate capabilities of LMMs. Existing benchmarks are often limited by narrow scenarios and specified tasks. To this end, we introduce CC-OCR, a comprehensive benchmark that possesses a diverse range of scenarios, tasks, and challenges. CC-OCR comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. We evaluate nine prominent LMMs and reveal both the strengths and weaknesses of these models, particularly in text grounding, multi-orientation, and hallucination of repetition. CC-OCR aims to comprehensively evaluate the capabilities of LMMs on OCR-centered tasks, driving advancement in LMMs.\nLeaderborad Gemini-1.5-Pro and Qwen2-VL-72B are the two top-performing models. Gemini-1.5-Pro achieves first in the multi-scene OCR, multilingual OCR, and document parsing tracks, while Qwen2-VL-72B takes first place in the KIE track and second place in the multi-scene OCR and document parsing tracks.\nBenchmark Introduction The CC-OCR benchmark is specifically designed to evaluate the OCR capabilities of multimodal large models. It draws from publicly available specialized task data, as well as carefully curated representative data from various real-world application scenarios. CC-OCR includes a range of core OCR tasks while also focusing on the challenges and difficulties that arise in applications. The benchmark covers most key perception tasks in the fields of image text and image documents. One of the philosophy behind this work is that high accuracy in perception is the foundation of multimodal cognition. CC-OCR comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, formula recognition, document parsing, table parsing, and key information extraction. It includes 39 subsets with 7,058 full annotated images, of which 41% are sourced from real applications, being released for the first time. This means that the benchmark can better assess the zero-shot recognition capabilities of large models.\nThe main features of CC-OCR include:\nFour OCR-centric Tasks: Multi-Scene Text Reading, Multilingual Text Reading, Document Parsing, and Visual Information Extraction; Fine-grained visual challenges: Multi-orientation, multi-scale, wild-scene noise, and various text fonts; Well annotated: For OCR, both textual labels and positional boxes are annotated. For Parsing, LaTeX and HTML formats are applicable for documents and tables respectively. For KIE, JSON format is adopted. Demo Cases In the following, we show the challenged examples of CC-OCR, along side with the prompt we used in the evaluation and the responses of top players. Example: Multi-Scene Text Recognition\rNext\rBlurred Receipt\rExample: Multi-Scene Text Recognition\rNext\rInverted can\rExample: Multi-Scene Text Recognition\rNext\rTexts with grounding\rExample: Document Parsing\rNext\rScreen-shot document\rExample: Document Parsing\rNext\rPhotoed table\rExample: Document Parsing\rNext\rHandwritten formula\rExample: Kie Information Extraction\rNext\rOpen layout\r","wordCount":"538","inLanguage":"en","datePublished":"2024-12-06T18:40:04+08:00","dateModified":"2024-12-06T18:40:04+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/cc-ocr-bench/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>From Pixels to Pages: Evaluating the Reading Powers of Large Models！</h1><div class=post-meta>&lt;span title='2024-12-06 18:40:04 +0800 CST'>December 6, 2024&lt;/span>&amp;nbsp;·&amp;nbsp;3 min&amp;nbsp;·&amp;nbsp;538 words&amp;nbsp;·&amp;nbsp;Qwen Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=http://localhost:1313/zh/blog/cc-ocr-bench/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://code.alibaba-inc.com/DamoAGI/CC-OCR class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://www.modelscope.cn/datasets/Qwen/CC-OCR/summary class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://arxiv.org/abs/2412.02210 class="btn external" target=_blank>Paper</a></p><p>The ability to read and write is a unique trait of humans and a hallmark of advanced intelligence. That&rsquo;s why it&rsquo;s becoming one of the essential skills that Large Multimodal Models ( LMMs ) must possess as they strive for human-like intelligence. LMMs have demonstrated impressive performance
in recognizing document images with natural language instructions. However, as we dive deeper into more real-world scenarios, we find that challenges still remain. How do we measure to what extent large models can effectively handle complex and richly detailed images that require fine perception? This is definitely a question worth exploring and pondering.</p><p>The current landscape lacks a comprehensive benchmark to effectively measure the literate capabilities of LMMs. Existing benchmarks are often limited by narrow scenarios and specified tasks. To this end, we introduce <strong>CC-OCR</strong>, a comprehensive benchmark that possesses a diverse range of scenarios, tasks, and challenges. <strong>CC-OCR</strong> comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. We evaluate nine prominent LMMs and reveal both the strengths and weaknesses of these models, particularly in text grounding, multi-orientation, and hallucination of repetition. <strong>CC-OCR</strong> aims to comprehensively evaluate the capabilities of LMMs on OCR-centered tasks, driving advancement in LMMs.</p><h1 id=leaderborad>Leaderborad<a hidden class=anchor aria-hidden=true href=#leaderborad>#</a></h1><figure><img src=https://img.alicdn.com/imgextra/i2/O1CN01B3N8lf1jfrPii5EoI_!!6000000004576-2-tps-4082-2113.png#center width=100%></figure><p>Gemini-1.5-Pro and Qwen2-VL-72B are the two top-performing models. Gemini-1.5-Pro achieves first in the multi-scene OCR, multilingual OCR, and document parsing tracks, while Qwen2-VL-72B takes first place in the KIE track and second place in the multi-scene OCR and document parsing tracks.</p><h1 id=benchmark-introduction>Benchmark Introduction<a hidden class=anchor aria-hidden=true href=#benchmark-introduction>#</a></h1><p>The <strong>CC-OCR</strong> benchmark is specifically designed to evaluate the OCR capabilities of multimodal large models. It draws from publicly available specialized task data, as well as carefully curated representative data from various real-world application scenarios. <strong>CC-OCR</strong> includes a range of core OCR tasks while also focusing on the challenges and difficulties that arise in applications. The benchmark covers most key perception tasks in the fields of image text and image documents. One of the philosophy behind this work is that high accuracy in perception is the foundation of multimodal cognition. <strong>CC-OCR</strong> comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, formula recognition, document parsing, table parsing, and key information extraction. It includes 39 subsets with 7,058 full annotated images, of which 41% are sourced from real applications, being released for the first time. This means that the benchmark can better assess the zero-shot recognition capabilities of large models.</p><figure><img src=https://img.alicdn.com/imgextra/i3/O1CN01Z4W4qP1ULVZvX3MPh_!!6000000002501-2-tps-4035-2080.png#center width=90%></figure><p>The main features of <strong>CC-OCR</strong> include:</p><ol><li><strong>Four OCR-centric Tasks</strong>: Multi-Scene Text Reading, Multilingual Text Reading, Document Parsing, and Visual Information Extraction;</li><li><strong>Fine-grained visual challenges</strong>: Multi-orientation, multi-scale, wild-scene noise, and various text fonts;</li><li><strong>Well annotated</strong>: For OCR, both textual labels and positional boxes are annotated. For Parsing, LaTeX and HTML formats are applicable for documents and tables respectively. For KIE, JSON format is adopted.</li></ol><h1 id=demo-cases>Demo Cases<a hidden class=anchor aria-hidden=true href=#demo-cases>#</a></h1><p>In the following, we show the challenged examples of <strong>CC-OCR</strong>, along side with the prompt we used in the evaluation and the responses of top players.<div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Multi-Scene Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Blurred Receipt</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i3/O1CN01aNiMTY1VzdtmO57TO_!!6000000002724-2-tps-1386-1006.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multi-Scene Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Inverted can</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN01p0449R1fLTJqRn0gL_!!6000000003990-2-tps-1384-1102.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multi-Scene Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Texts with grounding</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN011KZDTy1YProHA1Lcf_!!6000000003052-2-tps-1384-968.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Screen-shot document</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN01OGYW861cRTfMVwBep_!!6000000003597-2-tps-1384-1230.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Photoed table</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i2/O1CN01wQUvUy1gqRilSBb4H_!!6000000004193-2-tps-1392-1148.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Handwritten formula</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i1/O1CN01GVXfSZ1N7aE2VMZFZ_!!6000000001523-2-tps-1382-1528.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Kie Information Extraction</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Open layout</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN01cROwrT1iziyb6dlad_!!6000000004484-2-tps-1728-1522.png alt=image></div></div></div></div></p></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>