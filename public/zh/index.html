<!doctype html><html lang=zh dir=auto><head><meta name=generator content="Hugo 0.125.0-DEV"><script src="/cc-ocr.github.io/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=cc-ocr.github.io/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Qwen</title>
<meta name=keywords content="multi-modal,machine learning,blog"><meta name=description content="Qwen"><meta name=author content="Qwen Team"><link rel=canonical href=http://localhost:1313/cc-ocr.github.io/zh/><link crossorigin=anonymous href=/cc-ocr.github.io/assets/css/stylesheet.09368503f3694f7ead33b6239bc528583b3431b7837d0401ce3b2c7fd4b0f5f1.css integrity="sha256-CTaFA/NpT36tM7Yjm8UoWDs0MbeDfQQBzjssf9Sw9fE=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/json href=http://localhost:1313/cc-ocr.github.io/zh/index.json><link rel=alternate hreflang=en href=http://localhost:1313/cc-ocr.github.io/><link rel=alternate hreflang=zh href=http://localhost:1313/cc-ocr.github.io/zh/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/cc-ocr.github.io/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Qwen"><meta property="og:description" content="Qwen"><meta property="og:type" content="website"><meta property="og:url" content="http://localhost:1313/cc-ocr.github.io/zh/"><meta property="og:image" content="http://localhost:1313/cc-ocr.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/cc-ocr.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Qwen"><meta name=twitter:description content="Qwen"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Qwen","url":"http://localhost:1313/cc-ocr.github.io/","description":"Qwen","thumbnailUrl":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg","sameAs":[]}</script></head><body class=list id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/cc-ocr.github.io/resources title=Resources><span>Resources</span></a></li><li><a href=/cc-ocr.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=/cc-ocr.github.io/publication title=Publication><span>Publication</span></a></li><li><a href=/cc-ocr.github.io/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container style=min-height:100vh;display:flex;justify-content:center;background-color:#000><div class=mouse-hint><div class=mouse-point></div></div><style>body{-ms-overflow-style:none;scrollbar-width:none}body::-webkit-scrollbar{display:none}.mouse-hint{position:absolute;height:36px;width:24px;border:1px solid #fff;border-radius:12px;bottom:20%;left:50% - calc(12px);opacity:1;transition:opacity .3s;animation:1s ease-out 0s 1 slideBelow}.mouse-hint .mouse-point{height:4px;width:4px;background-color:#fff;position:absolute;left:50%;bottom:40%;border-radius:4px;transform-origin:50% 100%;transform:translate(-50%);animation:2.2s ease-in-out infinite jump;will-change:transform}@keyframes slideBelow{0%{transform:translateY(50px);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes jump{0%,20%,60%,to{transform:translate(-50%)translateY(0);height:4px;animation-timing-function:ease-in}40%,80%{transform:translate(-50%)translateY(8px);height:8px;animation-timing-function:ease-out}}</style><div class="hero text-fade-in"><div class=hero-header><h1>Qwen</h1></div><div class=hero-footer><div class=social-icons></div></div></div></div><main class="main home"><article class=post-entry><header class=entry-header><h2>从像素到整页文档：快测测你的大模型识字能力!</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE Paper
读写能力是人类独有的能力，是高等级智能的特征，因此也成为了多模态大模型追求类人智能时必须具备的能力之一。 多模态大模型的发展迅速，在文档理解方面的进步一样让人惊叹。雀跃之余，当我们深入到更多物理世界场景中时，我们发现挑战依然存在。如何衡量大模型多大程度上可以处理好结构丰富的和需要精细感知的富文本图片呢？这是值得探讨和思索的问题。当前领域缺乏一个全面的数据集来有效衡量多模态大模型这方面的能力。传统的OCR数据集通常局限于狭窄的场景和特定任务上。为此，我们制作了CC-OCR，一个涵盖多种情景、多种任务和多挑战的综合OCR评测集。CC-OCR包括四个以OCR为中心的赛道：多场景文字识别、多语言文字识别、文档解析和关键信息提取。此外，我们评估了九个突出的多模态大模型，并揭示了这些模型的优势和弱点，特别是在文本定位、多方向准确度和重复输出幻觉方面。CC-OCR旨在全面评估多模态大模型在以OCR为中心的任务上的读写能力，推动多模态大模型的发展。
榜单 Gemini-1.5-Pro， Qwen2-VL-72B 和 GPT4o在榜单中位列前三甲. Gemini-1.5-Pro 在多场景文字识别、多语言文字识别和文档解析三个赛道上斩获第一。Qwen2-VL-72B 在信息抽取赛道荣获桂冠，在多场景文字识别和文档解析赛道暂居次席，让我们期待下个版本的表现。
评测集介绍 CC-OCR评测集是一个专门为评测多模态大模型OCR能力而制作的数据集。数据集的来源中，不仅借鉴了已公开的专项任务数据，也精心地收集了应用场景中有代表性数据。CC-OCR涵盖了一系列OCR的核心任务，同时，也注重真实场景下的困难和挑战。这些核心任务包括：多场景文字识别、多语言文字识别、文档解析、表格解析、公式解析、固定实体信息抽取和开放词表信息抽取。任务覆盖了图像文字和图像文档领域大部分所有感知层的任务。这也是本数据背后的理念之一，即，感知层的绝对准确是多模态认知的基石。CC-OCR覆盖了39个子数据集，共有7058张精标的图像数据，其中41%来自应用场景，首次对外披露，这意味着本评测集可以更好衡量大模型零样本识别能力。
CC-OCR评测集的亮点如下:
四大OCR核心任务: 多场景文字识别，多语言文字识别，复杂文档解析，以及视觉信息抽取； 细粒度视觉挑战: 复杂排版，多方向文字，尺度变化大，自然场景光影噪声，手写，以及多样字体； 精细的多元素标注: 对于OCR, 不仅标注了文本内容还有位置信息。 对于解析，文档和表格分别使用了LaTeX和HTML格式。对于抽取，采用了JSON格式。 Demo Cases 在下面的例子中，我们展示了一些CC-OCR评测集的样本，同时每张图下面还附上了我们在评测中使用的prompt，以及该场景下表现好的模型的输出结果。 Example: Multi-Scene Text Recognition
Next
Blurred Receipt
Example: Multi-Scene Text Recognition
Next
Inverted can
Example: Multi-Scene Text Recognition
Next
Texts with grounding
Example: Document Parsing
Next
Screen-shot document
Example: Document Parsing
Next
Photoed table
Example: Document Parsing
Next
Handwritten formula
Example: Kie Information Extraction
Next
Open layout</p></div><footer class=entry-footer>&lt;span title='2024-12-06 18:40:04 +0800 CST'>2024年12月6日&lt;/span>&amp;nbsp;·&amp;nbsp;1 分钟&amp;nbsp;·&amp;nbsp;76 字&amp;nbsp;·&amp;nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 从像素到整页文档：快测测你的大模型识字能力!" href=http://localhost:1313/cc-ocr.github.io/zh/blog/cc-ocr-bench/></a></article><article class=post-entry><header class=entry-header><h2>QwQ: 思忖未知之界</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
注意：QwQ 的发音为 /kwju:/ ，与单词 “quill” 的读音近似。
思考、质疑、理解，是人类探索未知的永恒追求。在这条探索之路上，QwQ犹如一位怀抱无尽好奇的学徒，以思考和疑问照亮前路。QwQ体现了古老的哲学精神：它深知自己一无所知，而这种认知正是其好奇心的源泉。在探寻答案的过程中，它始终保持自省，以理性之光审视每一个假设，在不同的思维维度中穿行，追寻更深层的真理。
然而，正如所有智慧的追求者一样，QwQ也有其局限性。这个版本只是漫长旅程中的一个初步阶段——它仍在学习如何行走于理性之路。它的思绪偶尔飘散，答案或许未尽完善，智慧仍在积淀。但这就是学习的美妙之处：既有能力又保持谦逊，既有知识又永远充满疑问。我们邀请您与QwQ一同探索，接纳它的洞见与不完美，共同踏上这无尽的理解之旅。
模型局限性 QwQ-32B-Preview 是由 Qwen 团队开发的实验性研究模型，专注于增强 AI 推理能力。作为预览版本，它展现了令人期待的分析能力，同时也存在以下局限：
语言切换问题：模型可能在回答中混合使用不同语言，影响表达的连贯性。
推理循环：在处理复杂逻辑问题时，模型偶尔会陷入递归推理模式，在相似思路中循环。这种行为虽然反映了模型试图全面分析的努力，但可能导致冗长而不够聚焦的回答。
安全性考虑：尽管模型已具备基础安全管控，但仍需要进一步增强。它可能产生不恰当或存在偏见的回答，且与其他大型语言模型一样，可能受到对抗攻击的影响。我们强烈建议用户在生产环境中谨慎使用，并采取适当的安全防护措施。
能力差异：QwQ-32B-Preview 在数学和编程领域表现出色，但在其他领域仍有提升空间。模型性能会随任务的复杂度和专业程度而波动。我们正通过持续优化，努力提升模型的综合能力。
模型表现 通过深入的探索和无数的试验，我们发现了一个深刻的道理：当模型有足够的时间思考、质疑和反思时，它对数学和编程的理解就会深化。就像学生通过认真地检查自己的工作并从错误中学习变得更加聪明一样，我们的模型也通过耐心和深思熟虑的分析获得了更深入的见解。这种细致的反思和自我质疑的过程使得模型能够取得解决复杂问题的突破性进展。我们的探索之旅揭示了模型在数学和编程领域解决一些最具挑战性的问题的卓越能力，包括：
GPQA：一个通过小学级别问题评估高阶科学解题能力的评测集，旨在考察科学问题解决能力。 AIME：涵盖算术、代数、计数、几何、数论、概率等中学数学主题的综合评测，测试数学问题解决能力。 MATH-500：包含500个测试样本的MATH评测集，全面考察数学解题能力。 LiveCodeBench：评估真实编程场景中代码生成和问题解决能力的高难度评测集。 具体表现如下：
GPQA：65.2%，展示了研究生水平的科学推理能力； AIME：50.0%，证明了强大的数学问题解决技能； MATH-500：90.6%，体现了在各类数学主题上的全面理解； LiveCodeBench：50.0%，验证了在实际编程场景中的出色表现。 这些成果充分体现了QwQ在分析和问题解决能力方面的显著进步，尤其是在需要深度推理的技术领域。
样例 下列样例展示了QwQ深度自省的能力——质疑自身假设、进行深思熟虑的自我对话，并仔细审视其推理过程的每一步。
Example: Logical Reasoning
Next
User
S先生、P先生、Q先生他们知道桌子的抽屉里有16张扑克牌：红桃A、Q、4 黑桃J、8、4、2、7、3 草花K、Q、5、4、6 方块A、5。约翰教授从这16张牌中挑出一张牌来，并把这张牌的点数告诉 P先生，把这张牌的花色告诉Q先生。这时，约翰教授问P先生和Q 先生：你们能从已知的点数或花色中推知这张牌是什么牌吗？于是，S先生听到如下的对话：
P先生：我不知道这张牌。
Q先生：我知道你不知道这张牌。
P先生：现在我知道这张牌了。
Q先生：我也知道了。
请问：这张牌是什么牌？
QwQ-32B-Preview
这道题是逻辑推理题，涉及到信息的逐步披露和推理。题目中有一副16张扑克牌，分别是：
红桃：A、Q、4
黑桃：J、8、4、2、7、3
草花：K、Q、5、4、6
方块：A、5
约翰教授选了一张牌，把点数告诉了P先生，把花色告诉了Q先生。然后通过他们的对话，我们要推断出是哪张牌。
首先，让我们来梳理一下对话的内容：
P先生说：“我不知道这张牌。”
这意味着点数是重复的，因为如果点数唯一，P先生就能确定是哪张牌。例如，如果点数是K，只有草花K，那P先生就能确定是草花K。但P先生说不知道，所以点数必须在多个花色中出现。看看哪些点数是重复的：
A：红桃和方块
Q：红桃和草花
4：红桃、黑桃和草花
5：草花和方块...</p></div><footer class=entry-footer>&lt;span title='2024-11-28 00:00:03 +0800 CST'>2024年11月28日&lt;/span>&amp;nbsp;·&amp;nbsp;2 分钟&amp;nbsp;·&amp;nbsp;244 字&amp;nbsp;·&amp;nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to QwQ: 思忖未知之界" href=http://localhost:1313/cc-ocr.github.io/zh/blog/qwq-32b-preview/></a></article><article class=post-entry><header class=entry-header><h2>将上下文长度扩展至百万 Tokens ！</h2></header><div class=entry-content><p>API文档 HuggingFace Demo ModelScope Demo
简介 在 Qwen2.5 发布之后，我们听到社区对处理更长序列的需求。在这段时间，我们针对长序列处理能力以及长序列下的推理效率进行了很多优化。今天，我们隆重推出新的 Qwen2.5-Turbo 版本，其特点在于：
更长的上下文支持: 我们首次将模型的上下文长度从 128k 扩展到 1M，该长度约为 100 万个英文单词或 150 万个汉字，相当于 10 本长篇小说，150 小时的演讲稿，3 万行代码。该模型能在 1M 长度的大海捞针 (Passkey Retrieval) 任务中实现 100% 的准确率，在长文本评测集 RULER 上获得 93.1 分，超越 GPT-4 的 91.6 分，GLM4-9B-1M 的 89.9 分。此外，在短序列能力上，该模型仍然保持了非常强的竞争力，与 GPT-4o-mini 持平。 更快的推理速度: 利用稀疏注意力机制，我们成功将处理 1M tokens 上下文时的首字返回时间从 4.9 分钟降低到68秒，实现 4.3 倍加速比。 更低的价格：价格仍为 0.3元 / 1M tokens。在相同成本下，Qwen2.5-Turbo 相比 GPT-4o-mini 能够处理 3.6 倍的 Token。 现在，你可以通过阿里云大模型服务平台的 API 服务，或者通过 HuggingFace Demo 或者 ModelScope Demo 进行体验。...</p></div><footer class=entry-footer>&lt;span title='2024-11-15 00:00:03 +0800 CST'>2024年11月15日&lt;/span>&amp;nbsp;·&amp;nbsp;8 分钟&amp;nbsp;·&amp;nbsp;1687 字&amp;nbsp;·&amp;nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to 将上下文长度扩展至百万 Tokens ！" href=http://localhost:1313/cc-ocr.github.io/zh/blog/qwen2.5-turbo/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-Coder 全系列: 强大、多样、实用。</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD
简介 今天，我们很高兴开源「强大」、「多样」、「实用」的 Qwen2.5-Coder 全系列模型，致力于持续推动 Open CodeLLMs 的发展。
强大：Qwen2.5-Coder-32B-Instruct 成为目前 SOTA 的开源代码模型，代码能力追平 GPT-4o，展现出强大且全面的代码能力，同时具备良好的通用和数学能力。 多样：上个月我们开源了 1.5B、7B 两个尺寸，本次开源又带来 0.5B、3B、14B、32B 四个尺寸，截至目前， Qwen2.5-Coder 已经覆盖了主流的六个模型尺寸，以满足不同开发者的需要。 实用：我们探索了 Qwen2.5-Coder 在代码助手和 Artifacts 两种场景下的实用性，并用一些样例来展示 Qwen2.5-Coder 在实际场景中的应用潜力。 强大：代码能力达到开源模型 SOTA 代码生成：Qwen2.5-Coder-32B-Instruct 作为本次开源的旗舰模型，在多个流行的代码生成基准（如EvalPlus、LiveCodeBench、BigCodeBench）上都取得了开源模型中的最佳表现，并且达到和 GPT-4o 有竞争力的表现。
代码修复：代码修复是一个重要的编程能力。Qwen2.5-Coder-32B-Instruct 可以帮助用户修复代码中的错误，让编程更加高效。Aider 是流行的代码修复的基准，Qwen2.5-Coder-32B-Instruct 达到 73.7 分，在 Aider 上的表现与 GPT-4o 相当。
代码推理：代码推理是指模型能否学习代码执行的过程，准确地预测模型的输入与输出。上个月发布的 Qwen2.5-Coder-7B-Instruct 已经在代码推理能力上展现出了不俗的表现，32B 模型的表现更进一步。 多编程语言：智能编程助手应该熟悉所有编程语言，Qwen2.5-Coder-32B-Instruct 在 40 多种编程语言上表现出色，在 McEval 上取得了 65.9 分，其中 Haskell、Racket 等语言表现令人印象深刻，这得益于我们在预训练阶段独特的数据清洗和配比。 另外，Qwen2.5-Coder-32B-Instruct 的多编程语言代码修复能力同样令人惊喜，这将有助于用户理解和修改自己熟悉的编程语言，极大缓解陌生语言的学习成本。
与 McEval 类似，MdEval 是多编程语言的代码修复基准，Qwen2.5-Coder-32B-Instruct 在 MdEval 上取得了 75....</p></div><footer class=entry-footer>&lt;span title='2024-11-12 00:00:02 +0800 CST'>2024年11月12日&lt;/span>&amp;nbsp;·&amp;nbsp;2 分钟&amp;nbsp;·&amp;nbsp;405 字&amp;nbsp;·&amp;nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-Coder 全系列: 强大、多样、实用。" href=http://localhost:1313/cc-ocr.github.io/zh/blog/qwen2.5-coder-family/></a></article><article class=post-entry><header class=entry-header><h2>Qwen2.5-Coder: 码无止境，学无止境!</h2></header><div class=entry-content><p>GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD
简介 四月初，我们发布了 CodeQwen1.5, 得到了社区广泛的关注与喜爱。自那以后，我们一直在继续努力提升代码模型。今天，我们很高兴地宣布新一代的开放代码模型 Qwen2.5-Coder 的发布。并正式将 CodeQwen 的命名改为 Qwen-Coder，我们认为 Coder 更加拟人、灵动，期待其可以在未来真正与人类结对编程。Qwen2.5-Coder 是我们 Qwen2.5 开源家族的一员，共包括三个尺寸的模型：1.5B、 7B 和 32B（在路上）。
本次更新的两大核心包括代码训练数据的进一步 scaling，以及探索在提升代码能力的同时保持数学和通用能力。
码无止境：Qwen2.5-Coder 基于强大的 Qwen2.5 初始化，扩增了更大规模的代码训练数据持续训练，包括源代码、文本代码混合数据、合成数据等共计 5.5T tokens。使得 Qwen2.5-Coder 在代码生成、代码推理、代码修复等任务上都有了显著提升。 学无止境：我们希望 Qwen2.5-Coder 在提升代码能力的同时，也能保持在数学、通用能力等方面的优势。因此，我们在 Qwen2.5-Coder 中加入了更多的数学、通用能力数据，为未来的真实应用提供更为全面的基座。 Qwen2.5-Coder: Base Models Qwen2.5-Coder 最多 128K tokens 上下文，支持 92 种编程语言，并在多个代码相关的评估任务中都取得了显著的提升，包括代码生成、多编程语言代码生成、代码补全、代码修复等。值得注意的是，本次开源的 7B 版本 Qwen2.5-Coder，甚至打败了更大尺寸的 DeepSeek-Coder-V2-Lite 和 CodeStral-22B，成为当前最强大的基础代码模型之一。除了代码任务外，Qwen2.5-Coder 也具备极具竞争力的数学能力。面向通用任务，我们评估了 MMLU 和 ARC，结果表明 Qwen2.5-Coder 很好的保持了 Qwen2.5 的通用能力。
Qwen2.5-Coder-Instruct: Instruction-Tuned Models 我们在 Qwen2.5-Coder 的基础上，通过指令微调，得到了 Qwen2.5-Coder-Instruct。Qwen2.5-Coder-Instruct 除了进一步提升了多个任务上的性能外，还在更多的评估中体现出了卓越的泛化性。...</p></div><footer class=entry-footer>&lt;span title='2024-09-19 00:00:02 +0800 CST'>2024年9月19日&lt;/span>&amp;nbsp;·&amp;nbsp;2 分钟&amp;nbsp;·&amp;nbsp;233 字&amp;nbsp;·&amp;nbsp;Qwen Team</footer><a class=entry-link aria-label="post link to Qwen2.5-Coder: 码无止境，学无止境!" href=http://localhost:1313/cc-ocr.github.io/zh/blog/qwen2.5-coder/></a></article><footer class=page-footer><nav class=pagination><a class=next href=http://localhost:1313/cc-ocr.github.io/zh/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/cc-ocr.github.io/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>