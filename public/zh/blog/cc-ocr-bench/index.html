<!doctype html><html lang=zh dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>从像素到整页文档：快测测你的大模型识字能力! | Qwen</title>
<meta name=keywords content><meta name=description content="GITHUB HUGGING FACE MODELSCOPE Paper
读写能力是人类独有的能力，是高等级智能的特征，因此也成为了多模态大模型追求类人智能时必须具备的能力之一。 多模态大模型的发展迅速，在文档理解方面的进步一样让人惊叹。雀跃之余，当我们深入到更多物理世界场景中时，我们发现挑战依然存在。如何衡量大模型多大程度上可以处理好结构丰富的和需要精细感知的富文本图片呢？这是值得探讨和思索的问题。当前领域缺乏一个全面的数据集来有效衡量多模态大模型这方面的能力。传统的OCR数据集通常局限于狭窄的场景和特定任务上。为此，我们制作了CC-OCR，一个涵盖多种情景、多种任务和多挑战的综合OCR评测集。CC-OCR包括四个以OCR为中心的赛道：多场景文字识别、多语言文字识别、文档解析和关键信息提取。此外，我们评估了九个突出的多模态大模型，并揭示了这些模型的优势和弱点，特别是在文本定位、多方向准确度和重复输出幻觉方面。CC-OCR旨在全面评估多模态大模型在以OCR为中心的任务上的读写能力，推动多模态大模型的发展。
榜单 Gemini-1.5-Pro， Qwen2-VL-72B 和 GPT4o在榜单中位列前三甲. Gemini-1.5-Pro 在多场景文字识别、多语言文字识别和文档解析三个赛道上斩获第一。Qwen2-VL-72B 在信息抽取赛道荣获桂冠，在多场景文字识别和文档解析赛道暂居次席，让我们期待下个版本的表现。
评测集介绍 CC-OCR评测集是一个专门为评测多模态大模型OCR能力而制作的数据集。数据集的来源中，不仅借鉴了已公开的专项任务数据，也精心地收集了应用场景中有代表性数据。CC-OCR涵盖了一系列OCR的核心任务，同时，也注重真实场景下的困难和挑战。这些核心任务包括：多场景文字识别、多语言文字识别、文档解析、表格解析、公式解析、固定实体信息抽取和开放词表信息抽取。任务覆盖了图像文字和图像文档领域大部分所有感知层的任务。这也是本数据背后的理念之一，即，感知层的绝对准确是多模态认知的基石。CC-OCR覆盖了39个子数据集，共有7058张精标的图像数据，其中41%来自应用场景，首次对外披露，这意味着本评测集可以更好衡量大模型零样本识别能力。
CC-OCR评测集的亮点如下:
四大OCR核心任务: 多场景文字识别，多语言文字识别，复杂文档解析，以及视觉信息抽取； 细粒度视觉挑战: 复杂排版，多方向文字，尺度变化大，自然场景光影噪声，手写，以及多样字体； 精细的多元素标注: 对于OCR, 不仅标注了文本内容还有位置信息。 对于解析，文档和表格分别使用了LaTeX和HTML格式。对于抽取，采用了JSON格式。 Demo Cases 在下面的例子中，我们展示了一些CC-OCR评测集的样本，同时每张图下面还附上了我们在评测中使用的prompt，以及该场景下表现好的模型的输出结果。 Example: Multi-Scene Text RecognitionNextBlurred ReceiptExample: Multi-Scene Text RecognitionNextInverted canExample: Multi-Scene Text RecognitionNextTexts with groundingExample: Document ParsingNextScreen-shot documentExample: Document ParsingNextPhotoed tableExample: Document ParsingNextHandwritten formulaExample: Kie Information ExtractionNextOpen layout"><meta name=author content="Qwen Team"><link rel=canonical href=http://localhost:1313/zh/blog/cc-ocr-bench/><link crossorigin=anonymous href=/assets/css/stylesheet.09368503f3694f7ead33b6239bc528583b3431b7837d0401ce3b2c7fd4b0f5f1.css integrity="sha256-CTaFA/NpT36tM7Yjm8UoWDs0MbeDfQQBzjssf9Sw9fE=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/cc-ocr-bench/><link rel=alternate hreflang=zh href=http://localhost:1313/zh/blog/cc-ocr-bench/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="从像素到整页文档：快测测你的大模型识字能力!"><meta property="og:description" content="GITHUB HUGGING FACE MODELSCOPE Paper
读写能力是人类独有的能力，是高等级智能的特征，因此也成为了多模态大模型追求类人智能时必须具备的能力之一。 多模态大模型的发展迅速，在文档理解方面的进步一样让人惊叹。雀跃之余，当我们深入到更多物理世界场景中时，我们发现挑战依然存在。如何衡量大模型多大程度上可以处理好结构丰富的和需要精细感知的富文本图片呢？这是值得探讨和思索的问题。当前领域缺乏一个全面的数据集来有效衡量多模态大模型这方面的能力。传统的OCR数据集通常局限于狭窄的场景和特定任务上。为此，我们制作了CC-OCR，一个涵盖多种情景、多种任务和多挑战的综合OCR评测集。CC-OCR包括四个以OCR为中心的赛道：多场景文字识别、多语言文字识别、文档解析和关键信息提取。此外，我们评估了九个突出的多模态大模型，并揭示了这些模型的优势和弱点，特别是在文本定位、多方向准确度和重复输出幻觉方面。CC-OCR旨在全面评估多模态大模型在以OCR为中心的任务上的读写能力，推动多模态大模型的发展。
榜单 Gemini-1.5-Pro， Qwen2-VL-72B 和 GPT4o在榜单中位列前三甲. Gemini-1.5-Pro 在多场景文字识别、多语言文字识别和文档解析三个赛道上斩获第一。Qwen2-VL-72B 在信息抽取赛道荣获桂冠，在多场景文字识别和文档解析赛道暂居次席，让我们期待下个版本的表现。
评测集介绍 CC-OCR评测集是一个专门为评测多模态大模型OCR能力而制作的数据集。数据集的来源中，不仅借鉴了已公开的专项任务数据，也精心地收集了应用场景中有代表性数据。CC-OCR涵盖了一系列OCR的核心任务，同时，也注重真实场景下的困难和挑战。这些核心任务包括：多场景文字识别、多语言文字识别、文档解析、表格解析、公式解析、固定实体信息抽取和开放词表信息抽取。任务覆盖了图像文字和图像文档领域大部分所有感知层的任务。这也是本数据背后的理念之一，即，感知层的绝对准确是多模态认知的基石。CC-OCR覆盖了39个子数据集，共有7058张精标的图像数据，其中41%来自应用场景，首次对外披露，这意味着本评测集可以更好衡量大模型零样本识别能力。
CC-OCR评测集的亮点如下:
四大OCR核心任务: 多场景文字识别，多语言文字识别，复杂文档解析，以及视觉信息抽取； 细粒度视觉挑战: 复杂排版，多方向文字，尺度变化大，自然场景光影噪声，手写，以及多样字体； 精细的多元素标注: 对于OCR, 不仅标注了文本内容还有位置信息。 对于解析，文档和表格分别使用了LaTeX和HTML格式。对于抽取，采用了JSON格式。 Demo Cases 在下面的例子中，我们展示了一些CC-OCR评测集的样本，同时每张图下面还附上了我们在评测中使用的prompt，以及该场景下表现好的模型的输出结果。 Example: Multi-Scene Text RecognitionNextBlurred ReceiptExample: Multi-Scene Text RecognitionNextInverted canExample: Multi-Scene Text RecognitionNextTexts with groundingExample: Document ParsingNextScreen-shot documentExample: Document ParsingNextPhotoed tableExample: Document ParsingNextHandwritten formulaExample: Kie Information ExtractionNextOpen layout"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/zh/blog/cc-ocr-bench/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-12-06T18:40:04+08:00"><meta property="article:modified_time" content="2024-12-06T18:40:04+08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="从像素到整页文档：快测测你的大模型识字能力!"><meta name=twitter:description content="GITHUB HUGGING FACE MODELSCOPE Paper
读写能力是人类独有的能力，是高等级智能的特征，因此也成为了多模态大模型追求类人智能时必须具备的能力之一。 多模态大模型的发展迅速，在文档理解方面的进步一样让人惊叹。雀跃之余，当我们深入到更多物理世界场景中时，我们发现挑战依然存在。如何衡量大模型多大程度上可以处理好结构丰富的和需要精细感知的富文本图片呢？这是值得探讨和思索的问题。当前领域缺乏一个全面的数据集来有效衡量多模态大模型这方面的能力。传统的OCR数据集通常局限于狭窄的场景和特定任务上。为此，我们制作了CC-OCR，一个涵盖多种情景、多种任务和多挑战的综合OCR评测集。CC-OCR包括四个以OCR为中心的赛道：多场景文字识别、多语言文字识别、文档解析和关键信息提取。此外，我们评估了九个突出的多模态大模型，并揭示了这些模型的优势和弱点，特别是在文本定位、多方向准确度和重复输出幻觉方面。CC-OCR旨在全面评估多模态大模型在以OCR为中心的任务上的读写能力，推动多模态大模型的发展。
榜单 Gemini-1.5-Pro， Qwen2-VL-72B 和 GPT4o在榜单中位列前三甲. Gemini-1.5-Pro 在多场景文字识别、多语言文字识别和文档解析三个赛道上斩获第一。Qwen2-VL-72B 在信息抽取赛道荣获桂冠，在多场景文字识别和文档解析赛道暂居次席，让我们期待下个版本的表现。
评测集介绍 CC-OCR评测集是一个专门为评测多模态大模型OCR能力而制作的数据集。数据集的来源中，不仅借鉴了已公开的专项任务数据，也精心地收集了应用场景中有代表性数据。CC-OCR涵盖了一系列OCR的核心任务，同时，也注重真实场景下的困难和挑战。这些核心任务包括：多场景文字识别、多语言文字识别、文档解析、表格解析、公式解析、固定实体信息抽取和开放词表信息抽取。任务覆盖了图像文字和图像文档领域大部分所有感知层的任务。这也是本数据背后的理念之一，即，感知层的绝对准确是多模态认知的基石。CC-OCR覆盖了39个子数据集，共有7058张精标的图像数据，其中41%来自应用场景，首次对外披露，这意味着本评测集可以更好衡量大模型零样本识别能力。
CC-OCR评测集的亮点如下:
四大OCR核心任务: 多场景文字识别，多语言文字识别，复杂文档解析，以及视觉信息抽取； 细粒度视觉挑战: 复杂排版，多方向文字，尺度变化大，自然场景光影噪声，手写，以及多样字体； 精细的多元素标注: 对于OCR, 不仅标注了文本内容还有位置信息。 对于解析，文档和表格分别使用了LaTeX和HTML格式。对于抽取，采用了JSON格式。 Demo Cases 在下面的例子中，我们展示了一些CC-OCR评测集的样本，同时每张图下面还附上了我们在评测中使用的prompt，以及该场景下表现好的模型的输出结果。 Example: Multi-Scene Text RecognitionNextBlurred ReceiptExample: Multi-Scene Text RecognitionNextInverted canExample: Multi-Scene Text RecognitionNextTexts with groundingExample: Document ParsingNextScreen-shot documentExample: Document ParsingNextPhotoed tableExample: Document ParsingNextHandwritten formulaExample: Kie Information ExtractionNextOpen layout"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"http://localhost:1313/zh/blog/"},{"@type":"ListItem","position":2,"name":"从像素到整页文档：快测测你的大模型识字能力!","item":"http://localhost:1313/zh/blog/cc-ocr-bench/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"从像素到整页文档：快测测你的大模型识字能力!","name":"从像素到整页文档：快测测你的大模型识字能力!","description":"GITHUB HUGGING FACE MODELSCOPE Paper\n读写能力是人类独有的能力，是高等级智能的特征，因此也成为了多模态大模型追求类人智能时必须具备的能力之一。 多模态大模型的发展迅速，在文档理解方面的进步一样让人惊叹。雀跃之余，当我们深入到更多物理世界场景中时，我们发现挑战依然存在。如何衡量大模型多大程度上可以处理好结构丰富的和需要精细感知的富文本图片呢？这是值得探讨和思索的问题。当前领域缺乏一个全面的数据集来有效衡量多模态大模型这方面的能力。传统的OCR数据集通常局限于狭窄的场景和特定任务上。为此，我们制作了CC-OCR，一个涵盖多种情景、多种任务和多挑战的综合OCR评测集。CC-OCR包括四个以OCR为中心的赛道：多场景文字识别、多语言文字识别、文档解析和关键信息提取。此外，我们评估了九个突出的多模态大模型，并揭示了这些模型的优势和弱点，特别是在文本定位、多方向准确度和重复输出幻觉方面。CC-OCR旨在全面评估多模态大模型在以OCR为中心的任务上的读写能力，推动多模态大模型的发展。\n榜单 Gemini-1.5-Pro， Qwen2-VL-72B 和 GPT4o在榜单中位列前三甲. Gemini-1.5-Pro 在多场景文字识别、多语言文字识别和文档解析三个赛道上斩获第一。Qwen2-VL-72B 在信息抽取赛道荣获桂冠，在多场景文字识别和文档解析赛道暂居次席，让我们期待下个版本的表现。\n评测集介绍 CC-OCR评测集是一个专门为评测多模态大模型OCR能力而制作的数据集。数据集的来源中，不仅借鉴了已公开的专项任务数据，也精心地收集了应用场景中有代表性数据。CC-OCR涵盖了一系列OCR的核心任务，同时，也注重真实场景下的困难和挑战。这些核心任务包括：多场景文字识别、多语言文字识别、文档解析、表格解析、公式解析、固定实体信息抽取和开放词表信息抽取。任务覆盖了图像文字和图像文档领域大部分所有感知层的任务。这也是本数据背后的理念之一，即，感知层的绝对准确是多模态认知的基石。CC-OCR覆盖了39个子数据集，共有7058张精标的图像数据，其中41%来自应用场景，首次对外披露，这意味着本评测集可以更好衡量大模型零样本识别能力。\nCC-OCR评测集的亮点如下:\n四大OCR核心任务: 多场景文字识别，多语言文字识别，复杂文档解析，以及视觉信息抽取； 细粒度视觉挑战: 复杂排版，多方向文字，尺度变化大，自然场景光影噪声，手写，以及多样字体； 精细的多元素标注: 对于OCR, 不仅标注了文本内容还有位置信息。 对于解析，文档和表格分别使用了LaTeX和HTML格式。对于抽取，采用了JSON格式。 Demo Cases 在下面的例子中，我们展示了一些CC-OCR评测集的样本，同时每张图下面还附上了我们在评测中使用的prompt，以及该场景下表现好的模型的输出结果。 Example: Multi-Scene Text Recognition\rNext\rBlurred Receipt\rExample: Multi-Scene Text Recognition\rNext\rInverted can\rExample: Multi-Scene Text Recognition\rNext\rTexts with grounding\rExample: Document Parsing\rNext\rScreen-shot document\rExample: Document Parsing\rNext\rPhotoed table\rExample: Document Parsing\rNext\rHandwritten formula\rExample: Kie Information Extraction\rNext\rOpen layout\r","keywords":[],"articleBody":"GITHUB HUGGING FACE MODELSCOPE Paper\n读写能力是人类独有的能力，是高等级智能的特征，因此也成为了多模态大模型追求类人智能时必须具备的能力之一。 多模态大模型的发展迅速，在文档理解方面的进步一样让人惊叹。雀跃之余，当我们深入到更多物理世界场景中时，我们发现挑战依然存在。如何衡量大模型多大程度上可以处理好结构丰富的和需要精细感知的富文本图片呢？这是值得探讨和思索的问题。当前领域缺乏一个全面的数据集来有效衡量多模态大模型这方面的能力。传统的OCR数据集通常局限于狭窄的场景和特定任务上。为此，我们制作了CC-OCR，一个涵盖多种情景、多种任务和多挑战的综合OCR评测集。CC-OCR包括四个以OCR为中心的赛道：多场景文字识别、多语言文字识别、文档解析和关键信息提取。此外，我们评估了九个突出的多模态大模型，并揭示了这些模型的优势和弱点，特别是在文本定位、多方向准确度和重复输出幻觉方面。CC-OCR旨在全面评估多模态大模型在以OCR为中心的任务上的读写能力，推动多模态大模型的发展。\n榜单 Gemini-1.5-Pro， Qwen2-VL-72B 和 GPT4o在榜单中位列前三甲. Gemini-1.5-Pro 在多场景文字识别、多语言文字识别和文档解析三个赛道上斩获第一。Qwen2-VL-72B 在信息抽取赛道荣获桂冠，在多场景文字识别和文档解析赛道暂居次席，让我们期待下个版本的表现。\n评测集介绍 CC-OCR评测集是一个专门为评测多模态大模型OCR能力而制作的数据集。数据集的来源中，不仅借鉴了已公开的专项任务数据，也精心地收集了应用场景中有代表性数据。CC-OCR涵盖了一系列OCR的核心任务，同时，也注重真实场景下的困难和挑战。这些核心任务包括：多场景文字识别、多语言文字识别、文档解析、表格解析、公式解析、固定实体信息抽取和开放词表信息抽取。任务覆盖了图像文字和图像文档领域大部分所有感知层的任务。这也是本数据背后的理念之一，即，感知层的绝对准确是多模态认知的基石。CC-OCR覆盖了39个子数据集，共有7058张精标的图像数据，其中41%来自应用场景，首次对外披露，这意味着本评测集可以更好衡量大模型零样本识别能力。\nCC-OCR评测集的亮点如下:\n四大OCR核心任务: 多场景文字识别，多语言文字识别，复杂文档解析，以及视觉信息抽取； 细粒度视觉挑战: 复杂排版，多方向文字，尺度变化大，自然场景光影噪声，手写，以及多样字体； 精细的多元素标注: 对于OCR, 不仅标注了文本内容还有位置信息。 对于解析，文档和表格分别使用了LaTeX和HTML格式。对于抽取，采用了JSON格式。 Demo Cases 在下面的例子中，我们展示了一些CC-OCR评测集的样本，同时每张图下面还附上了我们在评测中使用的prompt，以及该场景下表现好的模型的输出结果。 Example: Multi-Scene Text Recognition\rNext\rBlurred Receipt\rExample: Multi-Scene Text Recognition\rNext\rInverted can\rExample: Multi-Scene Text Recognition\rNext\rTexts with grounding\rExample: Document Parsing\rNext\rScreen-shot document\rExample: Document Parsing\rNext\rPhotoed table\rExample: Document Parsing\rNext\rHandwritten formula\rExample: Kie Information Extraction\rNext\rOpen layout\r","wordCount":"76","inLanguage":"zh","datePublished":"2024-12-06T18:40:04+08:00","dateModified":"2024-12-06T18:40:04+08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/zh/blog/cc-ocr-bench/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>从像素到整页文档：快测测你的大模型识字能力!</h1><div class=post-meta>&lt;span title='2024-12-06 18:40:04 +0800 CST'>2024年12月6日&lt;/span>&amp;nbsp;·&amp;nbsp;1 分钟&amp;nbsp;·&amp;nbsp;76 字&amp;nbsp;·&amp;nbsp;Qwen Team&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=http://localhost:1313/blog/cc-ocr-bench/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://code.alibaba-inc.com/DamoAGI/CC-OCR class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://www.modelscope.cn/datasets/Qwen/CC-OCR/summary class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://arxiv.org/abs/2412.02210 class="btn external" target=_blank>Paper</a></p><p>读写能力是人类独有的能力，是高等级智能的特征，因此也成为了多模态大模型追求类人智能时必须具备的能力之一。 多模态大模型的发展迅速，在文档理解方面的进步一样让人惊叹。雀跃之余，当我们深入到更多物理世界场景中时，我们发现挑战依然存在。如何衡量大模型多大程度上可以处理好结构丰富的和需要精细感知的富文本图片呢？这是值得探讨和思索的问题。当前领域缺乏一个全面的数据集来有效衡量多模态大模型这方面的能力。传统的OCR数据集通常局限于狭窄的场景和特定任务上。为此，我们制作了<strong>CC-OCR</strong>，一个涵盖多种情景、多种任务和多挑战的综合OCR评测集。<strong>CC-OCR</strong>包括四个以OCR为中心的赛道：多场景文字识别、多语言文字识别、文档解析和关键信息提取。此外，我们评估了九个突出的多模态大模型，并揭示了这些模型的优势和弱点，特别是在文本定位、多方向准确度和重复输出幻觉方面。<strong>CC-OCR</strong>旨在全面评估多模态大模型在以OCR为中心的任务上的读写能力，推动多模态大模型的发展。</p><h1 id=榜单>榜单<a hidden class=anchor aria-hidden=true href=#榜单>#</a></h1><figure><img src=https://img.alicdn.com/imgextra/i2/O1CN01B3N8lf1jfrPii5EoI_!!6000000004576-2-tps-4082-2113.png#center width=100%></figure><p>Gemini-1.5-Pro， Qwen2-VL-72B 和 GPT4o在榜单中位列前三甲. Gemini-1.5-Pro 在多场景文字识别、多语言文字识别和文档解析三个赛道上斩获第一。Qwen2-VL-72B 在信息抽取赛道荣获桂冠，在多场景文字识别和文档解析赛道暂居次席，让我们期待下个版本的表现。</p><h1 id=评测集介绍>评测集介绍<a hidden class=anchor aria-hidden=true href=#评测集介绍>#</a></h1><p><strong>CC-OCR</strong>评测集是一个专门为评测多模态大模型OCR能力而制作的数据集。数据集的来源中，不仅借鉴了已公开的专项任务数据，也精心地收集了应用场景中有代表性数据。<strong>CC-OCR</strong>涵盖了一系列OCR的核心任务，同时，也注重真实场景下的困难和挑战。这些核心任务包括：多场景文字识别、多语言文字识别、文档解析、表格解析、公式解析、固定实体信息抽取和开放词表信息抽取。任务覆盖了图像文字和图像文档领域大部分所有感知层的任务。这也是本数据背后的理念之一，即，感知层的绝对准确是多模态认知的基石。<strong>CC-OCR</strong>覆盖了39个子数据集，共有7058张精标的图像数据，其中41%来自应用场景，首次对外披露，这意味着本评测集可以更好衡量大模型零样本识别能力。</p><figure><img src=https://img.alicdn.com/imgextra/i3/O1CN01Z4W4qP1ULVZvX3MPh_!!6000000002501-2-tps-4035-2080.png#center width=90%></figure><p><strong>CC-OCR</strong>评测集的亮点如下:</p><ol><li><strong>四大OCR核心任务</strong>: 多场景文字识别，多语言文字识别，复杂文档解析，以及视觉信息抽取；</li><li><strong>细粒度视觉挑战</strong>: 复杂排版，多方向文字，尺度变化大，自然场景光影噪声，手写，以及多样字体；</li><li><strong>精细的多元素标注</strong>: 对于OCR, 不仅标注了文本内容还有位置信息。 对于解析，文档和表格分别使用了LaTeX和HTML格式。对于抽取，采用了JSON格式。</li></ol><h1 id=demo-cases>Demo Cases<a hidden class=anchor aria-hidden=true href=#demo-cases>#</a></h1><p>在下面的例子中，我们展示了一些<strong>CC-OCR</strong>评测集的样本，同时每张图下面还附上了我们在评测中使用的prompt，以及该场景下表现好的模型的输出结果。<div class="full-width-container example-container"><div class=example-content><div class=title><span>Example: Multi-Scene Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Blurred Receipt</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i3/O1CN01aNiMTY1VzdtmO57TO_!!6000000002724-2-tps-1386-1006.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multi-Scene Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Inverted can</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN01p0449R1fLTJqRn0gL_!!6000000003990-2-tps-1384-1102.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Multi-Scene Text Recognition</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Texts with grounding</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN011KZDTy1YProHA1Lcf_!!6000000003052-2-tps-1384-968.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Screen-shot document</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN01OGYW861cRTfMVwBep_!!6000000003597-2-tps-1384-1230.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Photoed table</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i2/O1CN01wQUvUy1gqRilSBb4H_!!6000000004193-2-tps-1392-1148.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Document Parsing</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Handwritten formula</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i1/O1CN01GVXfSZ1N7aE2VMZFZ_!!6000000001523-2-tps-1382-1528.png alt=image></div></div></div><div class=example-content style=display:none><div class=title><span>Example: Kie Information Extraction</span>
<a class=next-button>Next</a></div><div class=grid-layout><div class=role>Open layout</div><div class=content><img loading=lazy src=https://img.alicdn.com/imgextra/i4/O1CN01cROwrT1iziyb6dlad_!!6000000004484-2-tps-1728-1522.png alt=image></div></div></div></div></p></div></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/zh/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>